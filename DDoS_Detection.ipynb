{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 22:30:06.941725: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-30 22:30:06.949451: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-30 22:30:07.013667: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-30 22:30:08.637752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/iwachan/DDoS_Detection/venv/lib/python3.10/site-packages/numpy/core/getlimits.py:542: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_path = \"./datasets/ddos_balanced/final_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(path, p=0.1):\n",
    "  df_dataset = pd.read_csv(path, skiprows=lambda i: i>0 and random.random() > p)\n",
    "\n",
    "  colToDrop = np.array(['Unnamed: 0','Flow ID', 'Src IP', 'Dst IP', 'Timestamp'])\n",
    "  df_dataset = df_dataset.drop(colToDrop, axis=1)\n",
    "  df_dataset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "  df_dataset.dropna(inplace=True)\n",
    "\n",
    "  df_dataset.replace(to_replace=\"Benign\", value=0, inplace=True)\n",
    "  df_dataset.replace(to_replace=\"ddos\", value=1, inplace=True)\n",
    "\n",
    "  return df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(df_dataset):\n",
    "       from plotly.offline import init_notebook_mode, iplot, plot\n",
    "       import plotly as py\n",
    "       import plotly.express as px\n",
    "       init_notebook_mode(connected=True)\n",
    "       import plotly.graph_objs as go\n",
    "\n",
    "       fig = go.Figure(data=[\n",
    "       go.Bar(name='Benign', \n",
    "              y=df_dataset[\"Label\"].value_counts().values[0:1],\n",
    "              x=['Benign'],\n",
    "              text = df_dataset[\"Label\"].value_counts()[0:1],\n",
    "              orientation='v',\n",
    "              textposition='outside',),\n",
    "       go.Bar(name='DDoS', \n",
    "              y=df_dataset[\"Label\"].value_counts().values[1:2],\n",
    "              x=['ddos'],\n",
    "              text = df_dataset[\"Label\"].value_counts()[1:2],\n",
    "              orientation='v',\n",
    "              textposition='outside',)\n",
    "       ])\n",
    "\n",
    "       fig.update_layout(\n",
    "                     width=800,\n",
    "                     height=600,\n",
    "                     title=f'Class Distribution',\n",
    "                     yaxis_title='Number of attacks',\n",
    "                     xaxis_title='Attack Name',)\n",
    "       iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train, test):\n",
    "       from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "       numerical_columns = ['Flow Duration', 'Tot Fwd Pkts',\n",
    "              'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
    "              'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
    "              'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
    "              'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
    "              'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n",
    "              'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
    "              'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
    "              'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags',\n",
    "              'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s',\n",
    "              'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean',\n",
    "              'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
    "              'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
    "              'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
    "              'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg',\n",
    "              'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
    "              'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts',\n",
    "              'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
    "              'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
    "              'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
    "              'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min']\n",
    "\n",
    "       min_max_scaler = MinMaxScaler().fit(train[numerical_columns])\n",
    "       train[numerical_columns] = min_max_scaler.transform(train[numerical_columns])\n",
    "       test[numerical_columns] = min_max_scaler.transform(test[numerical_columns])\n",
    "\n",
    "       return train, test\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_model(input_size):\n",
    "  neuralNetworkModel = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(input_size,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(rate = 0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(rate = 0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(rate = 0.3),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "  ])\n",
    "\n",
    "  print(neuralNetworkModel.summary())\n",
    "  return neuralNetworkModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_model():\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  return LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_model():\n",
    "  from sklearn.neighbors import KNeighborsClassifier\n",
    "  return KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianNB_model():\n",
    "  from sklearn.naive_bayes import GaussianNB\n",
    "  return GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_model():\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  return RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree_model():\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "  return DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, X_train, y_train, isDL=True):\n",
    "    if isDL:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=0.01)\n",
    "\n",
    "        early_stopping = callbacks.EarlyStopping(\n",
    "            monitor='loss', \n",
    "            min_delta = 0.001,\n",
    "            patience = 5,\n",
    "            restore_best_weights = True\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['binary_accuracy'],\n",
    "        )\n",
    "        history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size = 256,\n",
    "        callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        # summarize history for accuracy\n",
    "        plt.plot(history.history['binary_accuracy'])\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['train'], loc='upper left')\n",
    "        plt.show()\n",
    "        # summarize history for loss\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['train'], loc='upper left')\n",
    "        plt.show()\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_test, y_test):\n",
    "    from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n",
    "    import seaborn as sns\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.round(y_pred)\n",
    "\n",
    "    # Accuracy \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print('\\nAccuracy')\n",
    "    print(acc)\n",
    "\n",
    "    # Precision \n",
    "    prec = precision_score(y_test, y_pred)#,average='macro')\n",
    "    print('\\nPrecision')\n",
    "    print(prec)\n",
    "\n",
    "    # Recall\n",
    "    rec = recall_score(y_test, y_pred) #,average='macro')\n",
    "    print('\\nRecall')\n",
    "    print(rec)\n",
    "\n",
    "    # F1 score\n",
    "    f1 = f1_score(y_test,y_pred) #,average='macro')\n",
    "    print('\\nF1 Score')\n",
    "    print(f1)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print('\\nConfusion Matrix')\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cf_matrix, annot=True, fmt='g')\n",
    "\n",
    "    return acc, prec, rec, f1, cf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: no error message set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdata_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plot_class_distribution(df)\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mdata_processing\u001b[0;34m(path, p)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_processing\u001b[39m(path, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m   df_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m   colToDrop \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlow ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSrc IP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDst IP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m   df_dataset \u001b[38;5;241m=\u001b[39m df_dataset\u001b[38;5;241m.\u001b[39mdrop(colToDrop, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/DDoS_Detection/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DDoS_Detection/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DDoS_Detection/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/DDoS_Detection/venv/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: no error message set"
     ]
    }
   ],
   "source": [
    "df = data_processing(dataset_path)\n",
    "plot_class_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE_SEED = 1\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE_SEED)\n",
    "\n",
    "print(\"Full dataset:\\n\")\n",
    "print(\"Benign: \" + str(df[\"Label\"].value_counts()[[0]].sum()))\n",
    "print(\"DDoS: \" + str(df[\"Label\"].value_counts()[[1]].sum()))\n",
    "print(\"---------------\")\n",
    "\n",
    "print(\"Training set:\\n\")\n",
    "print(\"Benign: \" + str(train[\"Label\"].value_counts()[[0]].sum()))\n",
    "print(\"DDoS: \" + str(train[\"Label\"].value_counts()[[1]].sum()))\n",
    "y_train = np.array(train.pop(\"Label\"))\n",
    "X_train = train.values\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"---------------\")\n",
    "\n",
    "print(\"Test set:\\n\")\n",
    "print(\"Benign: \" + str(test[\"Label\"].value_counts()[[0]].sum()))\n",
    "print(\"DDoS: \" + str(test[\"Label\"].value_counts()[[1]].sum()))\n",
    "y_test = np.array(test.pop(\"Label\"))\n",
    "X_test = test.values\n",
    "print(\"X_test shape: \",X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = DNN_model(X_train.shape[1])\n",
    "dnn_model = compile_model(dnn_model, X_train, y_train)\n",
    "test_model(dnn_model, X_test ,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LR_model()\n",
    "lr_model = compile_model(lr_model, X_train, y_train, isDL=False)\n",
    "test_model(lr_model, X_test ,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNN_model()\n",
    "knn_model = compile_model(knn_model, X_train, y_train, isDL=False)\n",
    "test_model(knn_model, X_test ,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussianNB_model = GaussianNB_model()\n",
    "gaussianNB_model = compile_model(gaussianNB_model, X_train, y_train, isDL=False)\n",
    "test_model(gaussianNB_model, X_test ,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForest_model()\n",
    "random_forest_model = compile_model(random_forest_model, X_train, y_train, isDL=False)\n",
    "test_model(random_forest_model, X_test ,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model = DecisionTree_model()\n",
    "decision_tree_model = compile_model(decision_tree_model, X_train, y_train, isDL=False)\n",
    "test_model(decision_tree_model, X_test ,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
